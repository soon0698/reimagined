---
toc: true
layout: post
description: 
categories: [markdown]
title: CS294-158-SP20 Deep Unsupervised Learning Spring 2020 - (1)
---

> 본 포스팅은 버클리대학에서 진행된 Deep Unsupervised Learning 강의를 요약하고자 만들어진 포스팅이다. 강의에 대한 모든 자료와 영상은 [웹사이트]((https://sites.google.com/view/berkeley-cs294-158-sp20/home))에 공개되어 있으며, 강의자인 [Pieter Abbeel](https://people.eecs.berkeley.edu/~pabbeel/)은 [버클리 인공지능 연구 센터(BAIR)](https://bair.berkeley.edu/blog/?refresh=1)의 디렉터이기도 하다. 


[Course Website](https://sites.google.com/view/berkeley-cs294-158-sp20/home)
**Instructor:** [Pieter Abbeel](https://people.eecs.berkeley.edu/~pabbeel/)
**Syllabus**
**L1 (1/22) Introduction**
L2 (1/29) Autoregressive Models
L3 (2/5) Flow Models
L4 (2/12) Latent Variable Models
L5 (2/19) Implicit Models / Generative Adversarial Networks
L6 (2/26) Implicit Models / Generative Adversarial Networks (ctd) + Final Project Discussion
L7 (3/11) Self-Supervised Learning / Non-Generative Representation Learning
L8 (3/18) Strengths and Weaknesses of Unsupervised Learning Methods Covered Thus Far 
L9 (4/1) Semi-Supervised Learning; Unsupervised Distribution Alignment
L10 (4/8) Compression
L11 (4/15) Language Models -- Guest Instructor: Alec Radford (OpenAI)
L12 (4/29) Representation Learning in Reinforcement Learning

# Introduction
<!--https://www.youtube.com/watch?v=V9Roouqfu-M&feature=youtu.be-->
## What is Unsupervised Learning?
<!-->
//REF : 대표 자료 조사
-->
Unsupervised Learning(비지도학습)을 정말 간단히 요약하자면, 가공되지 않은 데이터로부터 패턴을 학습하는 것이다.<!--Capturing rich patterns in raw data with deep networks in a label-free way--> 기존의 지도학습이 데이터에 대한 정답(라벨)이 미리 요구된다면, 비지도학습은 그런 과정이 필요없는 장점을 갖고 있다. 현재의 비지도학습 방법론은 크게 2가지 범주로 분류할 수 있다.

- 생성 모델(Generative Models): 미가공 데이터에 대한 분포를 재구성하는 방법<!--recreate raw data distribution-->

생성 모델은 학습 데이터의 확률 분포를 학습하게 된다. 일단 이 확률 분포를 학습하여 구성하고나면 기존의 데이터를 분포에 맞추어 생성할 수도 있으며, 분포의 재구성을 통해 새로운 데이터 역시 생성하거나 추론할 수 있다. 이에 대해서는 강의를 진행하며 세부적인 내용들을 다룰 것이다.<!--https://minsuksung-ai.tistory.com/12-->

-  자기지도학습(Self-supervised Learning): 의미있는 정보의 이해가 요구되는 task를 해결하도록 학습하는 방법 <!--"puzzle" task that require semantic understanding-->

자기지도학습은 특정 task를 해결하기 위한 표현을 추출하여 학습한다. 예를 들어 임의로 [0, 90, 180, 270]도씩 회전시킨 이미지를 입력 영상으로 제공하였을 때, 회전된 이미지가 원본 이미지가 되기 위해서는 다시 얼마나 회전시켜야 하는지에 대한 task를 정의하여 task에 필요한 표현을 학습하고자 한 시도가 있다. 이 시도는 매우 간단한 일처럼 보이지만, 사실은 자기지도학습에서 아주 중요하고 심오한 연구 주제로 밝혀진 바 있다. [^1] <!--https://greeksharifa.github.io/self-supervised%20learning/2020/11/01/Self-Supervised-Learning/-->

## Ideal Intelligence

비지도학습이 각광받는 이유 중 하나로는 비지도학습이 이상적인 지능(Ideal Intelligence)의 형태에 가장 걸맞는 방법이라는 것이다. 그렇다면 이상적인 지능은 어떤 것을 갖추고 있어야 하는 것일까? 한마디로 정의하자면, 다음과 같다.

> "Ideal Intelligence" is all about compression (finding all patterns)

이상적인 지능은 결국 압축에 관한 것이다. 데이터에 대한 패턴을 찾는다는 것은 압축과 맞닿아있는 개념이다. **데이터에 내재된 패턴을 이해하고, 이를 더 간결한 형태로 바꿔쓰는 작업이기 때문이다.** 이러한 맥락으로 생각한다면, 다음과 같은 수학적 개념들이 제시될 수 있다.

- **Low Kolmogorov Complexity** (short description of raw data)

Kolmogorov Complexity[^2]는 주어진 데이터를 생성할 수 있는 가장 짧은 형태의 표현은 무엇인가? 라는 개념을 제시한다. 가장 짧은 형태의 표현을 찾아낼 수 있다면 우리는 주어진 데이터를 보다 효율적으로 압축할 수 있게 된다.

- **Solomonoff Induction** (optimal inference)
<!--http://www.scholarpedia.org/article/Algorithmic_probability-->
Solomonoff Induction은 Kolmogorov Complexity의 

You can use *italics*, **bold**, `code font text`, and create [links](https://www.markdownguide.org/cheat-sheet/). Here's a footnote [^1]. Here's a horizontal rule:

---

## Lists

Here's a list:

- item 1
- item 2

And a numbered list:

1. item 1
1. item 2

## Boxes and stuff

> This is a quotation

{% include alert.html text="You can include alert boxes" %}

...and...

{% include info.html text="You can include info boxes" %}

## Images

![]({{ site.baseurl }}/images/logo.png "fast.ai's logo")

## Code

You can format text and code per usual 

General preformatted text:

    # Do a thing
    do_thing()

Python code and output:

```python
# Prints '2'
print(1+1)
```

    2

Formatting text as shell commands:

```shell
echo "hello world"
./some_script.sh --option "value"
wget https://example.com/cat_photo1.png
```

Formatting text as YAML:

```yaml
key: value
- another_key: "another value"
```


## Tables

| Column 1 | Column 2 |
|-|-|
| A thing | Another thing |


## Tweetcards

{% twitter https://twitter.com/jakevdp/status/1204765621767901185?s=20 %}


## Footnotes



[^1]: S. Gidaris, P. Singh, and N. Komodakis, “Unsupervised Representation Learning by Predicting Image Rotations,” Arxiv, 2018.

[^2]: 